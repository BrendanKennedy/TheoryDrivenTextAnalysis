---
title: "Psyc 499: Text as Data"
subtitle: "Introduction to tf-idf, ngrams, and topic models"
author: "Joe Hoover"
date: "02.14.19 - Week 6 Lab"
output:
  html_notebook:
    toc: true
---

In this lab, we will continue our exploration of (mostly) tidy text analysis. 

First, let's prepare the environment. 

```{r}
library(tidyverse)
library(tidytext)
devtools::install_github("thomasp85/ggraph")
library(tidyr)
library(tidytext)
library(ggraph)
install.packages('tm')
install.packages('quanteda')


devtools::install_github("juliasilge/tidytext")
devtools::install_github("thomasp85/ggraph")


song_dat <- read_csv('songdata.csv') %>%
  select(-link)

```


## Word and document frequency

How can we tell what a document is _about_? 

* Look at its most frequent words?
* Look at its most frequent words, excluding stop words?
* ...?



<div class="alert alert-success">
  <strong>tf-idf</strong> Within-document term frequency weighted by the log of the total number of documents divided by the number of documents containing the term. 
</div>


```{r}

song_words <- song_dat %>%
  unnest_tokens(word, text) %>%
  count(artist, word, sort = TRUE) %>%
  ungroup()

total_words <- song_words %>% 
  group_by(artist) %>% 
  summarize(total = sum(n))

song_words <- left_join(song_words, total_words)

song_words
```

<div class="alert alert-success">
 What does the above tibble show? 
</div>


<div class="alert alert-success">
 What would we need, now, to calculate tf-idf?
</div>

#### tf-idf

```{r}
song_words <- song_words %>%
  bind_tf_idf(word, artist, n)

song_words
```

```{r}
song_words %>%
  arrange(artist, desc(tf_idf))
```
```{r}

library(wordcloud)

song_words %>%
  filter(artist %in% c('Jimi Hendrix')) %>%
  with(wordcloud(word, tf_idf, max.words = 100))

```


```{r}
library(reshape2)
library(scales)


song_words %>%
  filter(artist %in% c('Jimi Hendrix', 'The Beatles')) %>%
  group_by(artist) %>%
  top_n(40, tf_idf) %>%
  ungroup() %>%
  acast(word ~ artist, value.var = "tf_idf", fill = 0) %>%
  comparison.cloud(colors = c(scales::muted('red'), muted('blue')),
                   max.words = 100)

```




## N-grams

Are there any problems with using words as tokens?


### Extracting n-grams

```{r}

song_bigrams <- song_dat %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

song_bigrams

```

```{r}
bigrams_separated <- song_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```




```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```



```{r}
bigram_tf_idf <- bigrams_united %>%
  count(artist, bigram) %>%
  bind_tf_idf(bigram, artist, n) %>%
  arrange(artist,desc(tf_idf))

bigram_tf_idf
```

```{r}
bigram_tf_idf %>%
  filter(artist %in% c('Tool', 'Ariana Grande', 'Kanye West', 'Frank Sinatra')) %>%
  group_by(artist) %>%
  filter(rank(tf_idf) %in% 1:10) %>%
  ggplot(aes(x=reorder(bigram, desc(tf_idf)), y=tf_idf, fill=artist)) + 
  geom_col() +
  coord_flip()+
  facet_wrap(.~artist, scales='free') 
  
```



### Counting words within sections

```{r}


#devtools::install_github("juliasilge/tidytext")
library(widyr)
library(tidytext)
library(tidyverse)

beatles <- song_dat %>%
  filter(artist=='The Beatles') %>%
  unnest_tokens(word, text) %>%
  anti_join(., stop_words,by='word')


widyr::pairwise_count(beatles, word, song, sort = TRUE)


```

```{r}
word_cors <- beatles %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, song, sort = TRUE)

word_cors
```


```{r}

# devtools::install_github("thomasp85/ggraph")
# library(tidyr)
# library(tidytext)
# library(ggraph)

word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```


## Non-tidy formats 

<img src="https://www.tidytextmining.com/images/tidyflow-ch-5.png" alt="A flowchart of a typical text analysis that combines tidytext with other tools and data formats, particularly the tm or quanteda packages. This chapter shows how to convert back and forth between document-term matrices and tidy data frames, as well as converting from a Corpus object to a text data frame."/>

### Document-term matrix

**What is a document-term matrix (DTM)?**


#### DTM tidy tools

tidytext provides two functions for working with DTMs: 

* `tidy()`: convert DTM to tidy dataframe
* `cast()`: turns a tidy one-term-per-row data frame into a matrix. tidytext provides three variations of this verb, each converting to a different type of matrix: cast_sparse() (converting to a sparse matrix from the Matrix package), cast_dtm() (converting to a DocumentTermMatrix object from tm), and cast_dfm() (converting to a dfm object from quanteda).

### tm package

tm is one of the most widely used text analysis packages used in R. 


```{r}
library(tm)


```


```{r}
song_words.dtm <- song_words %>%
  cast_dtm(term=word, document=artist, value=n)


dim(song_words.dtm)
dim(song_words.dtm)

```


```{r}
song_words.dtm
```

```{r}
terms <- Terms(song_words.dtm)
head(terms)
```



```{r}
song_words.dtm <- song_words %>%
  anti_join(., stop_words) %>%
  cast_dtm(term=word, document=artist, value=n)


dim(song_words.dtm)
dim(song_words.dtm)

```





```{r}
song_words.dtm
```

```{r}
terms <- Terms(song_words.dtm)
head(terms)
```






```{r}
song_words2 <- song_words.dtm %>%
  tidy()


head(song_words2)

```


```{r}
# sparse matrix

song_words.sm <- song_words %>%
  cast_sparse(artist, word, n)

class(song_words.sm)

# Quanteda document-feature matrix
```

```{r}
song_words.dfm <- song_words %>%
  anti_join(., stop_words) %>%
  cast_dfm(term=word, document=artist, value=n)

song_words.dfm

```


## Linguistic Inquiry and Word Count(LIWC)

LIWC is a very popular program for counting words, which as we've seen is one way to conduct sentiment analysis. 

LIWC's methodology has been ported to R via the package `quanteda` and it is relatively easy to conduct a liwc analysis in R. 

```{r}
library(quanteda)
library(rvest)

#Specifying the url for desired website to be scraped
url <- 'https://www.moralfoundations.org/sites/default/files/files/downloads/moral%20foundations%20dictionary.dic'

#Reading the HTML code from the website
liwc_dic <- read_html(url) %>% 
  html_text()

fileConn<-file("mfd.dic")
writeLines(liwc_dic, fileConn)
close(fileConn)


mfd <- dictionary(file = "mfd.dic", 
                    format = "LIWC")

```

```{r}

song_words.mfd <- quanteda::dfm(song_words.dfm, dictionary = mfd) %>%
  tidy()

song_words.mfd

```

```{r}
song_words.mfd %>%
  filter(term %in% c('HarmVirtue', 'HarmVice','IngroupVirtue', 'IngroupVice')) %>%
  group_by(term) %>%
  top_n(10, wt=count) %>%
  ggplot(aes(x=reorder(document, count), y=count)) +
  geom_col() + 
  coord_flip() + 
  facet_wrap(.~term, scales='free', ncol=2)
  


```


```{r}

# https://github.com/dgrtwo/drlib/blob/master/R/reorder_within.R

reorder_within <- function(x, by, within, fun = mean, sep = "___", ...) {
  new_x <- paste(x, within, sep = sep)
  stats::reorder(new_x, by, FUN = fun)
}


#' @rdname reorder_within
#' @export
scale_x_reordered <- function(..., sep = "___") {
  reg <- paste0(sep, ".+$")
  ggplot2::scale_x_discrete(labels = function(x) gsub(reg, "", x), ...)
}


#' @rdname reorder_within
#' @export
scale_y_reordered <- function(..., sep = "___") {
  reg <- paste0(sep, ".+$")
  ggplot2::scale_y_discrete(labels = function(x) gsub(reg, "", x), ...)
}
```


```{r}
song_words.mfd %>%
  filter(term %in% c('HarmVirtue', 'HarmVice',
                     'IngroupVirtue', 'IngroupVice')) %>%
  left_join(total_words, by=c('document' = 'artist')) %>%
  mutate(rate = count/total) %>%
  group_by(term) %>%
  top_n(10, wt=rate) %>%
  ggplot(aes(x=reorder_within(document, by=rate, within=term), y=rate)) +
  geom_col() + 
  coord_flip() +
  scale_x_reordered() +
  facet_wrap(.~term, scales='free', ncol=2)
  
```




## Topic Modeling 

* Every document is a mixture of topics
* Every topic is a mixture of words


The goal of topic modeling is: 

* Given:
    * the above assumptions
    * a set of documents containing words
* Infer which words are associated with which topics
* AND infer which documents are associated with which documents


<img src="https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png" alt=""/>

* $\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions
* $\beta$ is the parameter of the Dirichlet prior on the per-topic word distribution
* $\theta_m$ is the topic distribution for document m
* $\phi_k$ is the word distribution for topic k
* $z_{mn}$ is the topic for the n-th word in document m
* $w_{mn}$ is the specific word.


### Fitting a LDA model

```{r}
# sudo apt-get install gsl-bin libgsl0-dev
install.packages('topicmodels')
library(topicmodels)

song_lda <- LDA(song_words.dtm, k = 10, control = list(seed = 1234))
song_lda
```


