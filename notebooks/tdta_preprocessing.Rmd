---
title: "Theory Driven Text Analysis Workshop"
subtitle: "Text Pre-processing\n\nSPSP 2020"
author: 
  name: "Joe Hoover & Brendan Kennedy"
  email: "joseph.hoover@kellogg.northwestern.edu\n\nbtkenned@usc.edu"
output:
  html_notebook:
    toc: yes
---


# Workshop Data

Throughout this workshop, we will be working with a collection of United States Congressional speeches from the House. This data is stored in JSONL format.

It is stored in the file: `../data/tdta_house_data.json`


# Loading data

```{r, echo=F, message=F, warning=F}
# Define chunk options
knitr::opts_chunk$set(echo=F, message=F, warning=F)
```

```{r, message=F,  echo=F}
# Load packages 
library(pacman)
p_load(readr, dplyr, tidyr, ggplot2, pmr, jtools, knitr, reshape2, jsonlite, 
       lubridate, stringr, tidytext, fst)
```


Text data can be stored in many formats:

* CSV
* Separate files (e.g. txt) for each "document"
* A single txt with documents newline delimited 
* JSON
* JSONL/NDJSON
* XML
* ...


With this much variation, the first challenge of text analysis is often figuring out how to load your data! Sometimes you know the format a priori, which simplifies the task. However, other times you might not know the format. In those instances, it's useful to take a glance at the raw data. 


We can do that with the `readLines` function: 





```{r, echo=T}
readLines('../data/tdta_house_data.json', n=1)
```

<br>

<div class="alert alert-success" role="alert">
  <strong>Question:</strong> Any idea what format this data is?
</div>



## JSON formatted data

### JSON

JSON is a data storage format that is widely used for storing text data. It has the following characteristics:

* Data is in name/value pairs
* Data is separated by commas
* Curly braces ("{" and "}") hold objects

For example, 

```{r, eval=F, echo=T}
# JSON eg 1
{'name': 'joe',
 'city': 'chicago'
} 
#JSON eg 2
{'name': c('joe', 'brendan'),
 'city': c('chicago', 'los angeles')
}
```

### JSONL/NDJSON

Often, a collection of data or a corpus of text is stored in multiple JSON objects that are delimited by a newline. For intsance, a corpus of 100 documents might be stored in a file with 100 json objects, one for each document, delimited by a new line. Data with this format is usually referred to as JSONL (e.g. JSON lines) or NDJSON (e.g. new line delimited JSON). 

For example, a file containing the json objects below would be a JSONL format data object:

```{r, eval=F, echo=T}
# JSON eg 1
{'name': 'joe', 'city': 'chicago', 'text': 'Hi'} 
{'name': 'brendan', 'city': 'los angeles', 'text': 'Howdy'} 
```


## Loading Workshop Data

Our data has a JSONL format. To load it, we will use the `stream_in` function from the `jsonlite` package. Specifically, we'll provide a file connection to our data file as the first argument to `stream_in`. 

The `stream_in` function will read the JSONL file and store it in an R data.frame object.


```{r, echo=T,message=F, results='hide'}
dh <- stream_in(file("../data/tdta_house_data.json"))
```

# Processing Workshop Data

First, let's try to get an idea of how the data is structured. 

```{r}
str(dh)
```


```{r}
summary(dh)
```


```{r}
cat(paste('Speaker: ', dh$speaker[1], '\n\nSegment: ', dh$segment[1], sep=''))
```

### Date Formatting


First, let's deal with the date field: 

```{r, echo=T}
head(dh$date)
```


<div class="alert alert-success" role="alert">
  <strong>Question:</strong> What format is this data?
</div>

We can use the `as_date` function from the `lubridate` package to convert the UNIX timestamps to human-readable date objects:

```{r, echo=T}
dhp <- dh %>%
  mutate(raw_date = date,
         date = lubridate::as_datetime(date))
head(dhp$date)
```

Wait...what's going on here? Obviously this isn't correct. What do we do?

Well, fortunately, each observation is associated with a URL. So, we can at least get check the correct date for the first record: 

```{r}
dh$url[1]
```

"https://www.congress.gov//congressional-record/1998/10/07/house-section/article/H9870-1"

It looks like the correct date for the first is October 07, 1998. With that information, we can figure out what the timestamp *should* be (roughly):


```{r, echo=T}
should_be = as.numeric(as.POSIXct("1998-10-07", format="%Y-%m-%d"))
is = dh$date[1]
cat(paste('Should be: ', should_be, ' (Digits = ', nchar(should_be),')\n', 
          'Is: ', is, ' (Digits = ', nchar(is),')\n', sep=''))
```

It looks like we're off by about three orders of magnitude. If we fix this, do we get the right date?

```{r, echo=T}
as_datetime(is/1000)
```

Let's try this for all the data and hand check a few cases:

```{r, echo=T}
dhp <- dh %>%
  mutate(raw_date = date,
         date = lubridate::as_datetime(date/1000))
dhp %>% select(date, url) %>% head()
```


## Text Pre-processing

### Regex

Next, we'll start to clean up our text data, which is contained in the `segments` field. First, let's look at a random document and see what we might need to clean:

```{r, echo=T}
dhp$segment[100]
```

At least in this post, there are numbers, new line characters, `\n`, and some meaningless punctuation, e.g. '\'. To do this, we'll use regex and the function `gsub` which uses regex pattern matching to find patterns and replace them with a specified set of characters. 


```{r, echo=T}
dhp <- dhp %>%
  mutate(text = gsub('\\\\|/', '', segment), # Remove '\' and '/'
         text = gsub('\\d+', 'NUM', text),
         text = gsub('\\\n|\\s+', ' ', str_trim(text)))
         
         
         
```

```{r, echo=T}
dhp$text[100]
```


Let's unpack our regex operation: 

* `gsub('\\\\|/', '', segment)` 
  * We want to remove '\' and '/'. However, '\' is an escape character. '\\' is R's version of a special regex character, and '\\\' escapes the regex character. So, to capture '\', we need to use '\\\\'. 
  * `|` functions as an OR operator. 
* `gsub('\\d+', 'NUM', text)`
  * `\\d` matches digit characters. Adding `+` matches sequences of digits.
  * Here, we replace digits with 'NUM'.
* `gsub('\\\n|\\s+', ' ', str_trim(text)))`


What else might we want to fix? Let's look at the beginning of a few documents:

```{r, echo=T}
substr(dhp$text[c(1:3, 400:403, 20000, 20002)], 0, 50)
```

<br>

<div class="alert alert-danger" role="alert">
  <strong>Problem:</strong> How could we remove these strings from each document?
</div>


```{r, echo=T}
# Add the correct regex patterns to remove 'Mr. Speaker,', 'Mr. Chairman,', 'Mrs. Speaker,' 'Mrs. Chairman,' and 'Madam Speaker,' 'Madam Chairman,'  
dhp <- dhp %>%
  mutate(text = gsub('', '', text))
         
```


```{r teacher_regex_code, echo=T}
# Add the correct regex patterns to remove 'Mr. Speaker,', 'Mr. Chairman,', 'Mrs. Speaker,' 'Mrs. Chairman,' and 'Madam Speaker,' 'Madam Chairman,'  
suffix = c('Speaker,', 'Chairman,', 'Chairwoman,')
mr = paste('Mr.', suffix)
mrs = paste('Mrs.', suffix)
intro_strs = paste(c(mr, mrs), collapse='|')
dhp <- dhp %>%
  mutate(text = gsub(intro_strs, '', text),
         text =str_trim(text))
         
```


```{r, echo=T}
substr(dhp$text[c(1:3, 400:403, 20000, 20002)], 0, 50)
```


Finally, if you look long enough, you'll notice quite a few HTML tags, like `<a href=\"billNUMth-congresshouse-billNUM\">`. Let's go ahead and remove those:


```{r, echo=T}
dhp <- dhp %>%
  mutate(text = gsub('<.*?>', '', text))
```


### `tidytext`

The notion of *tidy* data is quite popular among R users. 

Generally, there are three rules for tidy data:

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

How might these rules apply to text data? 


<div class="alert alert-danger">
  <strong>Problem</strong>  What do you think _tidy_ text is? Try to write down text-relevant versions of the three tidytext rules.
</div>

1. 
2. 
3. 

#### Converting documents into tidytext format

We can convert our data to tidytext format using the `unnest_tokens` function. First, let's go ahead and drop unnecessary columns and save our data: 

```{r}
dhp <- dhp %>%
  select(-segment, -url, -raw_date, -session) 
dhp %>%
  saveRDS('../data/tdta_clean_house_data.RDS')
```

Now, let's create a new dataframe with tidy text data using words as units. We'll also save this object for later use.

```{r, echo=T}
dat_tt_words <- dhp %>%
  unnest_tokens(word, text, token='words')


saveRDS(dat_tt_words, '../data/tdta_clean_house_data_tidy.RDS')

```


```{r}
dat_tt_words %>%
  select(speaker, word) %>% 
  head(8)
```

```{r}
print(substr(dhp$text[1],0, 50))
```

By converting our data to tidytext format, we have access to each word in a document *and* we can operate on them using standard tidyverse functions. Specifically:


* Now, each row corresponds to a word within a document
* Punctuation is automatically stripped
* Uppercase characters are lowered. 

As you'll see in the next module, this is particularly useful for dictionary-based analyses. But for now, let's try to get a handle on some of the basic characteristics of our data:

### Exploration and Visualization

#### Words across time

First, we'll look at word frequencies across time.


```{r}
dat_tt_words %>%
  group_by(date) %>%
  tally() %>%
  ggplot(aes(x = date, y = n/1000)) +
  geom_line() +
  theme_apa() +
  ggtitle('House Speech Word Count by Day') + 
  ylab('1k Words')
```

We can also look at word frequencies by time and party:

```{r}
dat_tt_words %>%
  group_by(date, Party) %>%
  tally() %>%
  ggplot(aes(x = date, y = n/1000)) +
  geom_line() +
  theme_apa() +
  ggtitle('House Speech Word Count by Day and Party') + 
  ylab('1k Words') + facet_wrap(Party~., ncol=1)
```


#### Top words

We can (and you always should) also glance at the word distributions. E.g., what are the top words?

```{r, echo=T}
dat_tt_words %>%
  count(word, sort = TRUE) %>%
  top_n(15, wt=n) %>%
  mutate(n = n/1000,
    word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() + 
  theme_apa() +
  ggtitle('Top words')
```

Looking at the top words, it's clear that the most frequent words are words like 'the', and 'of'. Depending on the task at hand, you may or may not want to remove these kinds of words, which are referred to as *stop words*. 

<div class="alert alert-success" role="alert">
  <strong>Question:</strong> When and why might you want to remove (or not remove) stop words?
</div>


We can easily remove stopwords using tidytext's stop_words dataset and an anti-join:

```{r}
head(stop_words)
```


```{r}
dat_tt_words.cl <- dat_tt_words %>%
  anti_join(stop_words) %>%
  filter(word != 'num')
```


```{r, echo=T}
dat_tt_words.cl %>%
  count(word, sort = TRUE) %>%
  top_n(15, wt=n) %>%
  mutate(n = n/1000,
    word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() + 
  theme_apa() +
  ggtitle('Top words with stopwords removed')
```



```{r, echo=T}
dat_tt_words.cl %>%
  filter(Party != 'Independent') %>%
  group_by(Party) %>%
  count(word, sort = TRUE) %>%
  top_n(15, wt=n) %>%
  mutate(n = n/1000,
    word = reorder(word, n)) %>%
  ggplot(aes(reorder(word,n), n)) +
  geom_col(position=position_dodge()) +
  xlab(NULL) +
  coord_flip() + 
  theme_apa() +
  ggtitle('Top words with stopwords removed by party') +
  facet_wrap(Party~., ncol=1)
```



```{r, echo=T}
dat_tt_words.cl %>%
  filter(Party != 'Independent') %>%
  group_by(Party) %>%
  count(word, sort = TRUE) %>%
  top_n(15, wt=n) %>%
  mutate(n = n/1000,
    word = reorder(word, n)) %>%
  ggplot(aes(reorder(word,n), n)) +
  geom_col(position=position_dodge()) +
  xlab(NULL) +
  coord_flip() + 
  theme_apa() +
  ggtitle('Top words with stopwords removed by party') +
  facet_wrap(Party~., ncol=1)
```




```{r}
`%!in%` <- Negate(`%in%`)
dat_tt_words.cl <- dat_tt_words.cl %>%
  filter(word  %!in% c('gentleman', 'bill', 'time', 'committee', 'section', 'people', 'amendment', 'house', 'act', 'united', 'congress'))
```



```{r, echo=T}

dat_tt_words.cl %>%
  filter(Party != 'Independent') %>%
  group_by(Party) %>%
  count(word, sort = TRUE) %>%
  top_n(15, wt=n) %>%
  mutate(n = n/1000,
    word = reorder(word, n)) %>%
  ggplot(aes(reorder(word,n), n)) +
  geom_col(position=position_dodge()) +
  xlab(NULL) +
  coord_flip() + 
  theme_apa() +
  ggtitle('Top words with stopwords removed by party') +
  facet_wrap(Party~., ncol=1)

```