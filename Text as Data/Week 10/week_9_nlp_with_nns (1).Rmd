---
title: "Psyc 499: Text as Data"
subtitle: "NLP with Neural Networks"
author: "Joe Hoover"
date: "02.28.19 - Week 9 Lab"
output:
  html_notebook:
    toc: true
---


```{r, message=F, results='hide'}
library(tidyverse)

if(require(knitr) == F) {
  install.packages('knitr')
  require('knitr')
}

if(require(textclean) == F) {
  install.packages('textclean')
  require('textclean')
}
require(textclean)


library(keras)


```




```{r}
library(tidyverse)
library(tidytext)
library(purrr)
library(readr)



get_imdb <- function(path){
  
  #path = "./imbd_data/aclImdb/train/pos/"
  
  fns <- list.files(path = path,
                            pattern = ".txt$")
  
  out <- fns %>%
    map_chr(~ read_file(file.path(path,.))) %>% 
        tibble(text = .)
  
}

train_pos <- get_imdb("./imbd_data/aclImdb/train/pos/") 
train_neg <- get_imdb("./imbd_data/aclImdb/train/neg/")

train_text.df <- rbind(train_pos, train_neg) %>% 
  mutate(y = c(rep(1, nrow(train_pos)), rep(0, nrow(train_neg))),
         text = replace_html(text, symbol = F), 
         text = gsub("([[:punct:]])", "", text),
         text = gsub("\\", "", text, fixed=T)) %>%
  sample_frac(1)

test_pos <- get_imdb("./imbd_data/aclImdb/test/pos/")
test_neg <- get_imdb("./imbd_data/aclImdb/test/neg/")
test_text <- rbind(test_pos, test_neg)

test_text.df <- rbind(test_pos, test_neg) %>% 
  mutate(y = c(rep(1, nrow(test_pos)), rep(0, nrow(test_neg))),
         text = replace_html(text, symbol = F), 
         text = gsub("([[:punct:]])", "", text),
         text = gsub("\\", "", text, fixed=T)) %>%
  sample_frac(1)

```

```{r}
train_text.df$y[[1]]
cat(train_text.df$text[[1]])
```



```{r}

train_docs <- train_text.df$text
max_features <- 20000
max_len <- 100
batch_size = 32
tokenizer <- text_tokenizer(num_words = max_features)

tokenizer %>% 
  fit_text_tokenizer(train_docs)
```


```{r}
tokenizer$document_count
head(tokenizer$word_index)
```


```{r}
tokenizer$word_index %>%
  head()
```



```{r}
text_seqs <- texts_to_sequences(tokenizer, train_docs)
text_seqs[1]
```

```{r}
train_docs[[1]]
tokenizer$word_index[10]
```


```{r}

x_train <- pad_sequences(
  text_seqs,
  padding = "post",
  maxlen = max_len
)

# x_train <- text_seqs %>%
#   pad_sequences(maxlen = maxlen)

dim(x_train)
```

```{r}
x_train[1,]

```

```{r}
y_train <- as.integer(train_text.df$y)
```


```{r}

model <- keras_model_sequential() %>% 
  layer_embedding(max_features, output_dim = 128, input_length = max_len) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units=250, activation = 'relu') %>%
  layer_dense(units = 1, activation = "sigmoid") 

```


```{r}

model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

```

```{r}

val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]
```


```{r}
history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 3,
  batch_size = batch_size,
  validation_data = list(x_val, y_val)
)


```



```{r}
plot(history)
```




# LSTM



```{r}

lstm_model <- keras_model_sequential() %>% 
  layer_embedding(max_features, output_dim = 128, input_length = max_len) %>%
  layer_lstm(units = 128, dropout=0.2, recurrent_dropout=0.2) %>%
  layer_dense(units = 1, activation = "sigmoid")

```


```{r}

lstm_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

```

```{r}

val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]
```


```{r}
history <- lstm_model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 3,
  batch_size = batch_size,
  validation_data = list(x_val, y_val)
)


```



```{r}
plot(history)
```



## 

